"""
M√≥dulo para limpieza de direcciones usando modelo T5 entrenado

Optimizaciones:
1. Lookup en Correccions.csv (instant√°neo, sin cargar modelo)
2. Cache de sesi√≥n para direcciones repetidas del mismo d√≠a
3. Solo carga el modelo si hay direcciones nuevas
"""
import os
from pathlib import Path
import re

# Variables globales
_model = None
_tokenizer = None
_lookup_dict = None
_lookup_loaded = False
_session_cache = {}  # Cache de direcciones procesadas en esta sesi√≥n


def _normalizar_key(texto):
    """Normaliza texto para b√∫squeda en lookup"""
    texto = texto.upper().strip()
    reemplazos = {'√Ä':'A','√Å':'A','√à':'E','√â':'E','√ç':'I','√í':'O','√ì':'O','√ö':'U','√ú':'U','√ë':'N','√á':'C'}
    for o, r in reemplazos.items():
        texto = texto.replace(o, r)
    return re.sub(r'\s+', ' ', texto)


def _cargar_lookup():
    """Carga solo el diccionario de lookup (r√°pido, sin modelo)"""
    global _lookup_dict, _lookup_loaded
    
    if _lookup_loaded:
        return _lookup_dict
    
    lookup_path = Path(__file__).parent.parent / "data" / "Correccions.csv"
    _lookup_dict = {}
    
    if lookup_path.exists():
        import pandas as pd
        df = pd.read_csv(lookup_path)
        df.columns = ['raw', 'processed']
        
        for _, row in df.iterrows():
            key = _normalizar_key(str(row['raw']))
            _lookup_dict[key] = row['processed']
        
        print(f"  üìö Lookup dict cargado: {len(_lookup_dict)} entradas conocidas")
    
    _lookup_loaded = True
    return _lookup_dict


def _cargar_modelo():
    """Carga el modelo T5 (solo cuando es necesario)"""
    global _model, _tokenizer
    
    if _model is not None:
        return _model, _tokenizer
    
    try:
        from transformers import T5Tokenizer, T5ForConditionalGeneration
        import torch
        
        model_path = Path(__file__).parent.parent / "models" / "address_model4"
        
        if not model_path.exists():
            raise FileNotFoundError(f"No se encontr√≥ el modelo en: {model_path}")
        
        print(f"  üì¶ Cargando modelo IA (para direcciones nuevas)...")
        
        _tokenizer = T5Tokenizer.from_pretrained("t5-small")
        _model = T5ForConditionalGeneration.from_pretrained(str(model_path))
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        _model = _model.to(device)
        _model.eval()
        
        print(f"  ‚úÖ Modelo cargado en {device.upper()}")
        
        return _model, _tokenizer
        
    except ImportError as e:
        print(f"  ‚ùå Error: Falta librer√≠a. Instala con: pip install transformers torch")
        raise e


def limpiar_direccion_con_modelo(direccion):
    """
    Limpia una direcci√≥n. Orden de prioridad:
    1. Cache de sesi√≥n (direcciones ya procesadas hoy)
    2. Lookup en Correccions.csv (instant√°neo)
    3. Modelo IA (solo si no se encuentra en anteriores)
    
    Args:
        direccion (str): Direcci√≥n sin procesar
        
    Returns:
        tuple: (direcci√≥n_limpia, fuente) donde fuente es 'cache', 'lookup' o 'modelo'
    """
    import torch
    global _session_cache
    
    key = _normalizar_key(direccion)
    
    # 1. Buscar en cache de sesi√≥n (direcciones repetidas del mismo d√≠a)
    if key in _session_cache:
        return _session_cache[key], 'cache'
    
    # 2. Buscar en lookup (Correccions.csv)
    lookup_dict = _cargar_lookup()
    if key in lookup_dict:
        resultado = lookup_dict[key]
        _session_cache[key] = resultado  # Guardar en cache
        return resultado, 'lookup'
    
    # 3. Usar modelo IA (carga el modelo si no est√° cargado)
    model, tokenizer = _cargar_modelo()
    device = next(model.parameters()).device
    
    input_text = "normalizar: " + direccion
    inputs = tokenizer(
        input_text, 
        return_tensors="pt", 
        max_length=256, 
        truncation=True
    ).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_length=128, 
            num_beams=4,
            early_stopping=True
        )
    
    resultado = tokenizer.decode(outputs[0], skip_special_tokens=True)
    _session_cache[key] = resultado  # Guardar en cache
    return resultado, 'modelo'


def procesar_direcciones_con_modelo(direcciones_raw, mostrar_comparativa=True):
    """
    Procesa una lista de direcciones usando lookup + modelo IA.
    Optimizado para evitar procesar direcciones repetidas.
    
    Args:
        direcciones_raw (list): Lista de direcciones sin procesar
        mostrar_comparativa (bool): Si True, imprime antes/despu√©s
        
    Returns:
        list: Lista de direcciones procesadas
    """
    global _session_cache
    _session_cache = {}  # Limpiar cache al inicio de cada procesamiento
    
    print("\n  ü§ñ Procesando direcciones...")
    
    # Primero cargar lookup (r√°pido)
    _cargar_lookup()
    
    direcciones_procesadas = []
    stats = {'cache': 0, 'lookup': 0, 'modelo': 0}
    
    if mostrar_comparativa:
        print("\n" + "="*80)
        print("  COMPARATIVA DE DIRECCIONES (RAW ‚Üí PROCESADA)")
        print("="*80)
    
    for i, direccion_raw in enumerate(direcciones_raw):
        direccion_procesada, fuente = limpiar_direccion_con_modelo(direccion_raw)
        direcciones_procesadas.append(direccion_procesada)
        stats[fuente] += 1
        
        if mostrar_comparativa:
            icono = {'cache': '‚ôªÔ∏è', 'lookup': 'üìö', 'modelo': 'ü§ñ'}[fuente]
            print(f"\n  [{i+1}] {icono} ANTES:  {direccion_raw}")
            print(f"      DESPU√âS: {direccion_procesada}")
    
    if mostrar_comparativa:
        print("\n" + "="*80)
    
    # Mostrar estad√≠sticas
    print(f"\n  üìä Estad√≠sticas de procesamiento:")
    print(f"     ‚ôªÔ∏è  Cache (repetidas): {stats['cache']}")
    print(f"     üìö Lookup (conocidas): {stats['lookup']}")
    print(f"     ü§ñ Modelo IA (nuevas): {stats['modelo']}")
    
    if stats['modelo'] == 0:
        print(f"     ‚ö° ¬°No fue necesario cargar el modelo IA!")
    
    return direcciones_procesadas
